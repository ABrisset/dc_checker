 
 
 
 
 
 
 ruby : web scraping avec la gem nokogiri • antoine brisset 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
         
  accueil  
  blog  
  contact  
     
  
  
 scraper facilement avec ruby et nokogiri 
     scripts seo  
  
     blog seo  >  scripts seo  
 publié le 15 apr 2013 
 le scraping est l'une des actions qui fait partie du quotidien d'un seo. on peut s'en servir par exemple en phase d'audit pour extraire le contenu de certaines balises, en phase de netlinking pour extraire les résultats google, etc. je vais vous présenter ici un petit script ruby réalisé avec l'aide de  @clement_ , et qui vous sera peut-être utile si vous n'avez pas sous la main un logiciel approprié. vous pourrez l'exécuter directement en console et récupérer ainsi rapidement ce dont vous avez besoin. 
 1ère étape : récupérer uniquement les données utiles du csv   tout d'abord, nous devons installer la gem “nokogiri” qui permet de parser et de scraper des documents en s'appuyant sur des sélecteurs css ou des expressions xpath. nous avons également besoin de la librairie “open-uri” et de la libraire “csv”, qui sont toutes deux des librairies standards de ruby.     #!/usr/bin/ruby 

 require   'rubygems' 
 require   'nokogiri' 
 require   'open-uri' 
 require   'csv' 
    nous récupérons dans un premier temps le contenu du csv dans un array à l'aide de la classe csv et de la méthode read, en spécifiant le chemin du fichier ainsi que le délimiteur utilisé, par exemple ici le point virgule.     array_of_arrays   =   csv  .  read  (  "./urls.csv"  ,   {  col_sep:   ";"  }) 
    il faut ensuite écrire la méthode qui permet de ne récupérer que le premier élément de chaque array de l'array global récupéré précédemment. autrement dit, uniquement la première cellule de chaque ligne de notre csv si notre csv comporte plusieurs colonnes.   pour cela, nous utilisons la méthode reduce. la méthode reduce permet, à partir d'un array, de retourner une valeur unique ou un array. nous allons dans un premier temps “passer” à reduce un tableau vide en valeur initiale ([]). puis initier la boucle.     array  .  reduce  ([])   do   |  result  ,   elem  | 
    a chaque passe, donc sur chaque élément de l'array (elem), la valeur totale est incrémentée (result). maintenant que nous avons découpé notre array, il ne reste plus qu'à demander à ruby de stocker dans “result” la première valeur de chaque “elem”, c'est-à-dire la première cellule de chaque ligne du fichier de base.     result   <<   elem  .  first 
    ce qui donne :     def   select_first_array_elem  (  array  ) 
     array  .  reduce  ([])   do   |  result  ,   elem  | 
         result   <<   elem  .  first 
     end 
 end 
    2ème étape : scraper sur la liste d'urls   il faut ensuite définir la méthode qui permet de récupérer les données recherchées dans le document html. c'est là qu'entre en scène  nokogiri . dans un premier temps, nous allons récupérer le contenu du document dans une variable.     def   analyse_url  (  url  ) 
   data   =   nokogiri  ::  html  (  open  (  url  )) 
    puis à partir de cette variable, récupérer le contenu du noeud qui nous intéresse. par exemple, la balise title.     title   =   data  .  xpath  (  "//title"  ).  text 
    et enfin, retourner le résultat. ce qui donne donc :     def   analyse_url  (  url  ) 
   data    =   nokogiri  ::  html  (  open  (  url  )) 
   title   =   data  .  xpath  (  "//title"  ).  text 
   title 
 end 
    3ème étape : boucler sur chaque url   une fois la méthode de scrape définie, il faut définir la méthode qui permet de boucler sur chaque élément de l'array obtenu en 1. nous déclarons une variable “result”, avec array vide. puis nous lançons la boucle (.each do). chaque élément “url” de la boucle devient un argument de la fonction de scrape précédente et l'ensemble est stocké dans “result”.     def   scraping_each_url  (  array  ) 
   result   =   [] 
   array  .  each   do   |  url  | 
     result   <<   analyse_url  (  url  ) 
   end 
   result 
 end 
    dernière étape : afficher les données récupérées   il ne reste plus qu'à afficher les données pour chaque url. pour cela rien de plus simple.on apelle les méthodes que l'on a définies.     array1    =   select_first_array_elem  (  array_of_arrays  ) 
 results   =   scraping_each_url  (  array1  ) 
    puis on affiche le résultat en console.     puts   "  #{  results  }  " 
    bien entendu, l'idéal est d'enregistrer le résultat dans un nouveau fichier csv en sortie.là non plus rien de sorcier et je vous laisse vous reporter à la  doc ruby  pour finir le travail ;)        comments powered by  disqus      
  
 
 
   saint andré lez lille 
 
   06 12 71 82 78 
 
    contact@antoine-brisset.com   
 
 
      
      
      
 
  © 2015 -  antoine brisset 
 
  
 
 
