 
 
 
 
 
 
 comparer deux sets d'urls avec ruby • antoine brisset 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
         
  accueil  
  blog  
  contact  
     
  
  
 comparer deux sets d'urls avec ruby 
     scripts seo  
  
     blog seo  >  scripts seo  
 publié le 11 aug 2013 
 dans le cadre d'une refonte, souvent, les urls d'un site sont modifiées et générées selon de nouvelles règles : ajout de répertoires, modification du séparateur d'url, renommage de catégories, etc. cela oblige donc à créer tout un paquet de redirections, afin d'assurer une transition correcte en termes de pr, d'indexation et de trafic moteur. je vous propose ici un petit script ruby, qui vous permettra de préparer le terrain en matchant vos anciennes urls avec les nouvelles... 
 préparer les deux jeux d'urls   dans un premier temps, il faut collecter toutes les urls existantes. pour faire cela, plusieurs méthodes complémentaires : crawl du site, scraping des pages indexées par google, export des landing pages les plus stratégiques via google analytics, etc. regroupez toutes ces urls dans un fichier, dédoublonnez, et sauvegardez en csv.   en parallèle, crawlez le nouveau site, avec votre outil homemade ou une solution comme screaming frog, et exportez toute les urls dans un deuxième fichier. glissez les deux fichiers dans un répertoire commun. vous voilà prêt pour la suite.   place au script   l'objectif du script est de décomposer chaque url du premier fichier (anciennes url), en ne conservant que le slug, puis de comparer chaque slug à chaque url du deuxième fichier, afin de déterminer quelles sont les urls du nouveau site qui contiennent des caractères communs avec chacun de ces slugs.   première étape : extraire les urls du csv et les placer dans un array   pour commencer, il faut faire appel aux gems csv et uri qui vous permettront de manipuler les fichiers csv ainsi que les urls comme vous le souhaitez.     #!/usr/bin/env ruby 

 require   'csv' 
 require   'uri' 
    ensuite, il faut extraire chaque url des fichiers csv et les placer dans un array. pour cela, on utilise la méthode “reduce” dont j'ai déjà parlé dans un article sur le  scraping .     def   select_first_array_elem  (  array  ) 
     array  .  reduce  ([])   do   |  result  ,   elem  | 
         result   <<   elem  .  first 
     end 
 end 
    deuxième étape : réduire chaque url à son slug   pour cela, nous allons définir une fonction utilisant “split”. cette méthode permet de découper une string sur la base d'un délimiteur spécifique. dans le cas d'une url, le délimiteur choisi sera donc le “/”. en sortie, un array sera généré avec chacun des éléments. dans notre cas, c'est uniquement la dernière partie de l'url, le slug, qui nous intéresse. nous allons donc sélectionner ce dernier élément de l'array avec un “.last”.   pour rendre plus propres les urls, nous allons ensuite utiliser la méthode “slice!” qui permet, à partir d'une string, de retourner une nouvelle string, dont on a supprimé certains caractères. ici, on utilise comme argument une regexp qui matche toutes les extensions possibles : html, gif, png, jpg, etc.     def   split_url  (  url  ) 
     result   =   url  .  split  (  '/'  ).  last 
     result  .  slice!  (  /.html|.gif|.jpg|.png/  ) 
     result 
 end 
    troisième étape : comparer chaque slug avec les nouvelles urls   dans cette partie, nous allons définir une fonction permettant, concrètement :     de prendre le slug de chaque url du premier fichier   de le comparer à chaque url du deuxième fichier   de créer un tableau avec l'url du 1er fichier et la (ou les) correspondance(s) trouvée(s) dans le 2ème fichier       def   included_in  (  array_1  ,  array_2  ) 
     # on initialise le tableau de sortie 
   results   =   array  .  new  (  0  ) 
     # on décompose chaque url du fichier contenant les anciennes urls 
     array_1  .  each   do   |  old_url  | 
         # on crée une variable avec l'host de de l'url 
         host   =   uri  .  parse  (  old_url  ).  host 
         # on extrait le slug de l'url 
         slug   =   split_url  (  old_url  ) 
         # on vérifie que l'url n'est pas égale à la racine 
         if   old_url   !=   "http://  #{  host  }  " 
             # on vérifie que le slug fait plus de 1 caractère 
             if   slug  .  length   >   1 
                 # on boucle sur chaque url du fichier contenant les nouvelles urls 
                 array_2  .  each   do   |  new_url  | 
                     # on crée de nouvelles variables uniquement si les nouvelles urls "contiennent" la variable "slug" 
                     if   new_url  .  include?  (  slug  ) 
                         url_from      =   old_url 
                         status        =   "matches" 
                         url_to        =   new_url 
                     end 
                     # on remplit le tableau avec ces variables 
                     results   <<   [  url_from  ,  status  ,  url_to  ] 
                 end 
             end 
         end 
     end 
   # on retourne le tableau dédoublonné 
     results  .  uniq 
 end 
    comme vous le voyez, j'utilise ici une méthode très pratique,  include?  qui permet de vérifier si un objet passé en argument est présent dans l'objet sur lequel on effectue le test.   quatrième étape : exporter les résultats dans un fichier csv   pour terminer, on construit un fichier csv. pour cela, nous allons donc utiliser la méthode csv. chaque ligne du tableau contiendra l'url source, la mention “matches” et l'url vers laquelle rediriger.     def   write_data_to_csv  (  path  ,   data  ) 
   csv  .  open  (  path  ,   "wb"  )   do   |  csv  | 
     csv   <<   [  "url from"  ,   "status"  ,   "url to redirect"  ] 
     data  .  each   do   |  elem  | 
         csv   <<   elem 
         end 
     end 
 end 
    il ne reste plus qu'à déclarer le chemin des fichiers et à utiliser nos différentes fonctions.     # chemin vers le fichier de résultats 
 file_path         =   "./results.csv" 

 # création d'un array avec les fichiers d'entrées (anciennes urls, nouvelles urls) 
 arr_of_arrs       =   csv  .  read  (  "./array_1.csv"  ) 
 arr_of_arrs_2     =   csv  .  read  (  "./array_2.csv"  ) 

 # sélection de la première colonne de chaque fichier d'entrée (urls) 
 array_1               =   select_first_array_elem  (  arr_of_arrs  ) 
 array_2               =   select_first_array_elem  (  arr_of_arrs_2  ) 

 # création d'un array sur la base des slugs du fichier array_1 retrouvés dans array_2 
 union                 =   included_in  (  array_1  .  uniq  ,  array_2  .  uniq  ) 

 # export en csv 
 write_data_to_csv  (  file_path  ,   union  ) 
    et voilà, si vous êtes sous apache, vous n'avez plus qu'à faire un rechercher/remplacer avec “rewriterule” et les flags 301 qui vont bien :)        comments powered by  disqus      
  
 
 
   saint andré lez lille 
 
   06 12 71 82 78 
 
    contact@antoine-brisset.com   
 
 
      
      
      
 
  © 2015 -  antoine brisset 
 
  
 
 
